"""DeepEval tests for {{ skill_name }}.

Run: agentura test {{ domain }}/{{ skill_name }}
Or:  pytest tests/test_deepeval.py
"""

import json
from pathlib import Path

import pytest
from deepeval import assert_test
from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric
from deepeval.test_case import LLMTestCase


FIXTURES_DIR = Path(__file__).parent.parent / "fixtures"


def load_fixture(name: str) -> dict:
    return json.loads((FIXTURES_DIR / name).read_text())


@pytest.fixture
def sample_input():
    return load_fixture("sample_input.json")


class TestSkillRelevancy:
    """Verify skill output is relevant to input."""

    def test_output_relevancy(self, sample_input):
        metric = AnswerRelevancyMetric(threshold=0.7)
        test_case = LLMTestCase(
            input=json.dumps(sample_input),
            actual_output="Replace with actual skill output after running",
            retrieval_context=[],
        )
        assert_test(test_case, [metric])


class TestSkillFaithfulness:
    """Verify skill doesn't hallucinate beyond input context."""

    def test_output_faithfulness(self, sample_input):
        metric = FaithfulnessMetric(threshold=0.7)
        test_case = LLMTestCase(
            input=json.dumps(sample_input),
            actual_output="Replace with actual skill output after running",
            retrieval_context=[json.dumps(sample_input)],
        )
        assert_test(test_case, [metric])
